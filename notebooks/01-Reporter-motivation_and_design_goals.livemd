<!-- livebook:{"autosave_interval_s":60} -->

# Reporter: Goals, motivation, methodology and design

```elixir
Mix.install([
  {:decimal, "~> 2.3"}
])
```

## Abstract

_Reporter_ is an application designed for the production of professional enterprise reports. A critical and expected component of software applications performing important functions in businesses is the generation of dynamic PDF reports, for printing or sharing with other people. What is key is that reports must be customisable, aesthetically pleasing, professionally typeset, and print-ready, in case of PDF production, or consumption-ready in cases of other office-suite reports needing to be consumed as an intermediate format.

<!-- livebook:{"break_markdown":true} -->

_Reporter_ is a report-generation wrapper around the Jasper Reports library. The intent behind this project is to completely sidestep the need to program in Java for generating professional reports. Clojure, a Lisp family of languages, is built on Java and hosted on that virtual machine, which gives it access to decades of enterprise-level applications that run on the Java virtual machine, while _improving_ on a number of Java's rough edges, opening it up to a pleasing programming environment—to those who love working in Lisp.

## The everlasting need for reports

Reports are the lifeblood of a corporation, finding use in board discussions where it is unwieldy to rally participants around a shared computer screen, or to present interactions with an application via a projection; in doling out tasks to employees and subcontractors; in synthesising thoughts and plans of action, over carefully formatted and distilled figures on a printed report, without the distraction of the rest of the application, a means to achieve higher thought processes. There are many reasons not covered by this list, but the point stands: reports are not going away and must be given the respect they deserve in professional settings.

<!-- livebook:{"break_markdown":true} -->

There are many types of report needed, depending on the activity in question, the person or people the report targets, the action which the report should instigate, and many more. Below is a non-exhausive list of reports needed on a regular basis:

<!-- livebook:{"break_markdown":true} -->

* Invoices
* Proof of receipt
* Statements of transactions
* List of outstanding payments, monies owed
* Financial statements, company accounts
* Sales reports
* Marketing performance reports
* Customer relationship management reports
* Lists of tasks outstanding
* Lists of actions undertaken, work carried out
* List of schedules, e.g. regular maintenance, consumable replacement, safety checks due
* Inventories taken and recorded on particular dates
* Summary of communications with third parties
* Specifications of a product, service or asset
* Marketing materials for the above
* Resource allocation map, e.g. available properties, potential tenants
* Manifest
* List of assets

## Report-generating solutions considered

Report generation is a very important function of all serious software efforts. The most common requirement is the creation of PDF reports, but a few related formats are often desirable in commercial settings, including creation of comma-separated values (CSV) exports, word processor documents (Word, Pages, LibreOffice Writer), spreadsheets (Excel, Numbers, LibreOffice Calc), presentations (PowerPoint, Keynote), as well as others.

<!-- livebook:{"break_markdown":true} -->

In consideration of plausible solutions to the requirement, the most important criteria had to be met:

* A visual report template designer is a must, so that non-developers could use this to modify existing templates for their needs
* An API for generating finished documents
* Support for dynamic content generation
* Rich text formatting capabilities
* Generation of PDF as a minimum, as well as some of the target document types described above
  * That the PDF generation isn't merely a glorified HTML to PDF solution, such as `wkhtmltopdf`, as HTML isn't yet a powerful typesetting and layout solution
  * Pixel-perfect design precision
  * Page-aware layout engine
  * Print-oriented typesetting
    * Quality typesetting, preferrably with high-quality justification
    * Different first pages
    * Document structure, including:
      * Different page sizes within the same document
      * Section-specific headers and footers
      * Page numbering that can reset between sections
      * Background layers that can vary by page type
  * Print-ready output (desirable, not mandatory)
    * CMYK color space (important for professional printing)
    * Embedded fonts
    * High-resolution image embedding
    * PDF/A compliance for archiving
    * Proper bleed and margin handling
  * Predictable rendering
* Reduced resource use, higher-performance report generation

<!-- livebook:{"break_markdown":true} -->

Solutions considered:

1. Jasper Reports
2. Pentaho
3. BIRT
4. QuestPDF

<!-- livebook:{"break_markdown":true} -->

Any solution to be considered had to meet a few desiderata to be taken seriously:

* The project is open-source
* The project is alive and healthy, as witnessed by the recency and frequency of commits to their repository
* A usable level of documentation exists, so that potential users are not expected to chase for basic information in forums and issues

<!-- livebook:{"break_markdown":true} -->

The underlying expectation in the selection process was that any solution chosen was likely to be in a very foreign programming language, most likely to be written in one of the corporate _darlings_: Java, .NET, C#, JavaScript, and would have to be _adapted_ in some way to work with in-house preferred languages: Elixir, Clojure, Common Lisp, OCaml. This drove the decision to wrap the generator within one of the in-house languages.

<!-- livebook:{"break_markdown":true} -->

I selected Jasper Reports as it clearly meets many of the criteria laid out; its visual designer Jaspersoft Studio was fairly easy to use intuitively; it had a comprehensive project manual with which to get started. As Jasper is written in Java, Clojure is a pragmatic choice as the wrapper language, as I understand it and trust a number of the guarantees it provides, and the developer experience it affords.

## Initial report prototype

One of the important reasons driving the choice for Jasper is that it has a graphical user interface application—Jaspersoft Studio—making it possible for non-developers to design and modify reports with their drag and drop interface.

<!-- livebook:{"break_markdown":true} -->

To demonstrate the feasibility of Jaspersoft Studio, a sample report template was designed, a client timesheet, defined to take in a file containing a list of JSON-formatted data records, for use as the report's data source. Entirely using Studio, the template was designed, the datasource registered, and the feasibility of the graphical interface proved itself fully capable, making it easy to generate incremental finished reports through the _Preview_ facility. Rounding it off, Studio makes it trivial to export the template under design, using the defined datasource, directly to one of fourteen different formats.

<!-- livebook:{"break_markdown":true} -->

In the early prototyping stage, Studio made it easy to finesse the demonstration timesheet report, finally exporting it to PDF, all from the visual user interface. The relative simplicity of the entire prototyping step proved that Jasper—as a report-generating solution—meets all pragmatic project needs, determining that the entire solution would be architected around the Jasper Reports library.

## Inter-process communication

With the initial hypothesis now proven, that Jasper Reports can generate pleasing and rich PDF reports, the next step is to start building a solution _wrapping_ all available functionality in a language and environment more amenable to me. Clojure is my answer to wrapping Java libraries, given that the language is hosted on the Java virtual machine. The question needing an answer now is how a requesting process would formally make the initial request—monitoring its progress and outcome—of the report-generating solution?

<!-- livebook:{"break_markdown":true} -->

Several methods are available:

1. Local communication (e.g., file-based or HTTP server) works well for simple, local use cases.
2. Distributed communication using HTTP APIs, message queues, or GRPC is more scalable and suitable for remote or cloud-based environments.
3. Containers provide flexibility and ease of scaling with orchestration tools like Docker Compose or Kubernetes.

I am not a fan of containers, so will not further two method 3. Breaking the other two methods down provides further finer choices of direction:

1. Local Interprocess Communication (IPC)

   1. **File-based Communication** The requesting application can write data to a file (such as JSON or CSV) and invoke the report-generating service, which reads the file, generates the report, and returns it. This is useful for simple, one-off reporting tasks.

      **Pros**:

      * Simple to implement for small-scale, local tasks.
      * Easy to debug and monitor.

      **Cons**:

      * Not very scalable.
      * Could lead to data synchronization issues in high-frequency or real-time scenarios.

   2. **Local HTTP Server** The report-generating service can expose an HTTP API using libraries like Ring or http-kit, and the requesting application can make HTTP requests to this local service.

      **Pros**:

      * Easy to scale out (you can add more report-generating instances behind a load balancer).
      * Can be easily monitored and controlled.

      **Cons**:

      * More complex than file-based communication.
      * Slightly more overhead with HTTP, but still suitable for local use.

2. Remote Communication (Distributed Services)

   1. **HTTP API (RESTful or GraphQL)** Expose a RESTful API or GraphQL endpoint in the report-generating application, where the requesting application can send HTTP requests to pass the data. This is the most flexible and scalable solution, as it allows you to separate concerns and deploy each service independently.

      **Pros**:

      * Highly scalable: You can run multiple instances of the report-generating service.
      * Can be made stateless and cloud-friendly.
      * Easily monitored using common observability tools.

      **Cons**:

      * Requires more infrastructure (e.g., load balancers, monitoring, etc.).

   2. **Message Queue (e.g., RabbitMQ, Kafka, etc.)** The requesting application can send messages (data for the report) to a message queue, and the report-generating can consume these messages to generate reports. This allows for asynchronous communication, better decoupling, and scaling. This works well if you need to queue multiple requests or manage high volumes of requests.

      **Pros**:

      * Asynchronous and scalable.
      * Handles retries and message persistence.
      * Works well for high-volume use cases.

      **Cons**:

      * More infrastructure required (e.g., RabbitMQ, Kafka).
      * Slightly more complex setup.

   3. **GRPC (For High-Performance, Low-Latency Communication)** For highly performant, low-latency communication, you can use gRPC (a remote procedure call framework). Both Elixir and Clojure have gRPC libraries that allow for bidirectional communication between services.

<!-- livebook:{"break_markdown":true} -->

The variable is the scale and requirements of the application, here are some options:

1. Local communication (e.g., file-based or HTTP server) works well for simple, local use cases.
2. Distributed communication using HTTP APIs, message queues, or GRPC is more scalable and suitable for remote or cloud-based environments.

<!-- livebook:{"break_markdown":true} -->

Judging the requirements on their own merit, the gRPC approach is technically _the best_, with the local file-based method a more pragmatic second.

<!-- livebook:{"break_markdown":true} -->

I'm a strong believer in simplicity, and while the distributed option has many benefits—distribution and ability to scale onto different and multiple systems, as well as the inherent volume and capacity benefits of that approach—the complexity of running this as a distributed service is not justifiable for all but the very most demanding environments. The local solution is better on many levels, including through simplified management, rollout and continued maintenance of the solution. I believe that this simplicity and the time savings to be reaped from it far outweigh the startup time of the application each time a report is requested, the _warmup_ of the solution, and the time taken for the delivery of the report. What's more, I think that the expectation—and need—for instant reports, simply doesn't exist; for the few reports that are needed (as a fraction of all functionality delivered) a reasonable waiting time is perfectly acceptable.

<!-- livebook:{"break_markdown":true} -->

The way forward, then, is to develop a local communication method, using the filesystem or similar method, to handle the volume that is expected, and in the future this solution can be extended to also support distributed communication, distribution and scaling. Though GRPC is seductive, the simpler HTTP method is better understood and supported across all architectural levels of the application.

## Local inter-process communication strategy and its benefits

#### Communication complexity

* **Complexity** A file-based approach is relatively simple to implement as it relies on basic file I/O operations (reading and writing to disk). The requesting application would write data to a file (in JSON, CSV, or other consumable format), and the report-generating application would read that file, generating the report, writing the output to a file which will be picked up by the requesting application.
* **Batch Processing** This approach is not “real-time” in the way that a gRPC method of communication would afford. Data is first written to a file, then later processed, making this less suitable for low-latency or real-time applications.
* **Asynchronous Nature** File-based communication is naturally asynchronous, as the calling application can spawn new processes independently handling report generation. However, this requires proper synchronization or polling to ensure the calling application knows when the report is ready.
* **Scalability** File-based communication doesn’t inherently scale out across multiple services, containers or servers, and its scalability would depend on how the file I/O system is designed and managed (e.g., shared network drive, cloud storage, etc.).

<!-- livebook:{"break_markdown":true} -->

#### System design and workflow

* **Loose Coupling** File-based communication is typically more loosely coupled. The requesting and generating applications don’t need to be aware of each other directly, beyond the files they exchange. The requesting application can write reports asynchronously, and the report-generating application can process them when available.
* **Process Independence** The requesting application can run report generation processes in parallel (by spawning multiple processes) and check files at intervals for completion, allowing the services to work independently.
* **No Dependency on Running Services** If the report-generating application isn’t running at the moment, the requesting application can still generate files or trigger report generation jobs, providing some flexibility. If the system goes down, it’s easier to restart the processes without a tight dependency chain.

<!-- livebook:{"break_markdown":true} -->

#### Error handling and failures

* **Error Handling** In a file-based system, error handling would likely be manual. If something goes wrong, you’ll need to handle retries or notify the system to take corrective actions.
* **Fault Tolerance** File-based approaches may be more tolerant to failures in the sense that both the requesting and generating applications can fail independently without causing a complete system failure. If the generator crashes, the requestor can wait and retry the file generation process.
* **Logging** Logging and monitoring in a file-based system can be more challenging. You need to ensure proper logging of file operations, and may need a manual or custom process to track file generation status (e.g., polling the filesystem).

<!-- livebook:{"break_markdown":true} -->

#### Performance and latency

* **Higher Latency** File-based communication inherently involves higher latency due to disk I/O and file reading/writing. The report generation process depends on file availability and may involve some polling or file system watching, which takes longer than the immediate RPC approach of gRPC.
* **Synchronous/Asynchronous Blocking** the requesting application will be blocked while waiting for the report generation process to complete if polling is used. If the file I/O system is slow, it can add additional delays.

<!-- livebook:{"break_markdown":true} -->

#### Scalability and high availability

* **Limited Scalability** Scaling with file-based communication requires careful consideration of file storage (e.g., shared network storage, cloud-based storage) and managing access to these files concurrently. You might need a distributed file system or cloud file storage to handle multiple instances of the report generation process.
* **Backup Systems** For remote backup, file-based systems are easy to implement by copying files to another service, but it may not be as seamless or efficient as the automatic failover provided by a gRPC-based system.

<!-- livebook:{"break_markdown":true} -->

#### Infrastructure and Maintenance

* **Minimal Infrastructure** A file-based approach requires minimal infrastructure. At minimum, a shared file system (local or cloud-based) is needed, where the requesting application can drop the data for the report-generating application to pick it up. If the system is small, you can run it without additional network or server configuration.
* **Scaling via Filesystem** Once you need to scale, use cloud storage services (e.g., AWS S3) or a network-attached storage (NAS) device to hold the files.

<!-- livebook:{"break_markdown":true} -->

#### Key Advantages of File-based Approach Over gRPC for Failure Management

* **Failure isolation** In the file-based approach, the requesting and generating applications can fail independently. If the generator fails, the requestor can simply restart it without being blocked or needing complex error handling. On the other hand, in gRPC, if the generating service goes down, the requestor will be impacted by this failure until the service is restored.
* **Decoupled failure handling** The file-based approach provides much more decoupling between the applications. If one process fails (either of the applications), the other can continue running, and the requestor can handle errors by either retrying or spawning new processes independently. gRPC, however, relies on real-time connectivity between both services, so failure in one service immediately affects the other.
* **Minimal external infrastructure needed** With file-based communication, you don’t need to implement complex monitoring systems (like Prometheus, health checks, etc.). The requesting application can directly manage the generator process lifecycle. For gRPC, you’d typically require additional tools for health checks, load balancing, or retries to make sure communication is resilient.
* **Simplicity** File-based communication is often simpler to reason about. With file I/O, failures tend to be easier to debug because you’re working with persistent files, and recovery or retries are less likely to depend on network connectivity issues.

<!-- livebook:{"break_markdown":true} -->

In conclusion, if the use case involves relatively few reports per day, minimal infrastructure, and asynchronous processing, the file-based approach is simpler to implement and maintain, and is sufficient. However, if you expect future growth, need low-latency and scalable communication, or plan to integrate with other systems, gRPC is a better long-term solution.

<!-- livebook:{"break_markdown":true} -->

File-based communication is a more loosely coupled and fault-tolerant approach. The requesting application has complete control over spawning and managing report—generating application—processes, making it simpler to handle failures, retries, and reinitializing processes when needed.

<!-- livebook:{"break_markdown":true} -->

In the case of failure recovery, the file-based approach excels by providing the requesting application with full autonomy over process management, including detecting failures and restarting processes as needed. The gRPC approach would require external monitoring and retry mechanisms for resiliency.

## Modifying local file-based communication to database-based

While file-based inter-process communication is simple and standard across many systems, it is different enough that it will impose per-operating system implementation modifications, due to differences—however small—between BSD, Linux, Darwin and Windows file handling subtleties. A very good substitution for file-based communication is an abstraction existing acros all the above systems and more: SQLite, the embedded relational database system. SQLite already powers billions of devices, and its role is proven in even beating native file system performance, so this research direction has significant merits, not least of which is that it presents a unified interface, _regardless_ of operating system.

Using a database-backed job queue instead of file-based communication has other significant advantages while still preserving the benefits of fault tolerance, monitoring, and process control.

<!-- livebook:{"break_markdown":true} -->

By using a separate SQLite database as a queue for pending report generation jobs, we gain:

* **Decoupling between the applications** The requesting and generating applications interact indirectly through the database instead of needing real-time connectivity like gRPC.
* **Failure resilience** Jobs persist in the queue even if either of the applications crashes, resulting in  no lost jobs.
* **Monitoring and control** The calling application can inspect the queue at any time, retry or remove failed jobs.
* **Concurrency and scaling** Multiple instances of the generating application can pick up jobs in parallel for distributed processing.
* **No file system dependencies** Unlike the file-based approach, the database avoids file-level locks, race conditions, and cleanup issues.

<!-- livebook:{"break_markdown":true} -->

#### Architecture of the SQLite Job Queue Solution

High-level overview of the report-generation workflow:

1. Job Insertion (requesting application):
   * The requesting application inserts a job (report request) into the jobs table in a dedicated SQLite database.
   * It sets the job status as "pending".
2. Job Processing (generating application):
   * The report-generation service polls the database at regular intervals.
   * It picks up the next available job, marks it as "processing", and generates the report.
   * Once done, it updates the job with the generated report path and marks it as "completed".
3. Result Retrieval (requesting application):
   * The requesting application checks for completed jobs and retrieves the results.
   * It can retry failed jobs, delete processed jobs, or trigger notifications when a report is ready.
4. Error Handling & Recovery:
   * If the report-generating process crashes, jobs remain in "pending" or "processing" status.
   * Requesting applications can detect stalled jobs (e.g., by using a timestamp column) and retry or reassign them.

<!-- livebook:{"break_markdown":true} -->

Below is a simple starting schema for the job queue table:

```sql
CREATE TABLE report_jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    status TEXT CHECK(status IN ('pending', 'processing', 'completed', 'failed')) NOT NULL DEFAULT 'pending',
    parameters TEXT NOT NULL,  -- JSON payload of report parameters
    result_path TEXT,          -- Path to generated report (filled by Clojure)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

<!-- livebook:{"break_markdown":true} -->

### Failure Scenarios & Resilience

Report-generator service fails mid-processing

* If the generator crashes while processing, the job remains in "processing" state.
* The requesting application can detect stalled jobs (e.g., based on a timestamp) and reset them back to “pending”.

<!-- livebook:{"break_markdown":true} -->

```sql
UPDATE report_jobs SET status = 'pending' WHERE status = 'processing' AND updated_at < datetime('now', '-5 minutes');
```

<!-- livebook:{"break_markdown":true} -->

Requesting service crashes

* Jobs in the queue persist in the SQLite database. And potentially beyond, as an audit trail.
* Once the requesting service restarts, it can continue handling results as usual.

Multiple report generator instances (scaling)

* Multiple report generation processes can compete for jobs.
* The first process to update the status to "processing" _wins_ the job.

<!-- livebook:{"break_markdown":true} -->

```sql
UPDATE report_jobs SET status = 'processing' WHERE id = ? AND status = 'pending';
```

<!-- livebook:{"break_markdown":true} -->

* This ensures only one instance processes a job at a time.

<!-- livebook:{"break_markdown":true} -->

### Advantages of a database compared to file-based communication

| Feature           | File-Based Approach              | gRPC-Based Approach                         | SQLite Job Queue Approach                |
| ----------------- | -------------------------------- | ------------------------------------------- | ---------------------------------------- |
| Fault Tolerance   | Moderate (crash = job loss risk) | Low (needs external monitoring)             | High (jobs persist in DB)                |
| Monitoring        | Requestor can track process      | Needs external tooling                      | Requestor can query DB                   |
| Scalability       | Low (single process)             | Moderate (gRPC load balancing)              | High (multiple workers)                  |
| Requestor Control | Full (spawns processes)          | Minimal (relies on report generator uptime) | Full (Requestor controls DB & job retry) |
| Complexity        | Low                              | High (gRPC infra required)                  | Moderate (DB management)                 |

<!-- livebook:{"break_markdown":true} -->

### Summary

The SQLite-based job queue idea is a robust, fault-tolerant, and scalable alternative to both gRPC and file-based communication.

* It maintains asynchronous processing while ensuring jobs persist in case of failure.
* It provides monitoring & control directly through Elixir without needing extra infrastructure.
* It allows the requesting application to detect failures and restart report-generating instances if needed.

## Local inter-process data sharing

With the [embedded] database route chosen, it's important to decide on the _data interchange_ method. There are two obvious ways to do this: (a) JSON-based (or other similar data-interchange format); and (b) table-based interchange. Despite only naming JSON as the data interchange format in method _a_, many popular—better and worse—formats exist, and are equally usable in this situation, including EDN, XML, CSV, MessagePack, direct database query, and others. Regardless of format, once the data is encoded, based on the choice of using a local database file as inter-process communication strategy, this encoded input dataset will be passed along to the report-generating table as a text field _within_ the job request table. In method _b_, the interchange is via a _dedicated database table_, which needs to be created, holding the input dataset for the duration of the process. Let's analyse the tradeoffs in these two options.

<!-- livebook:{"break_markdown":true} -->

### Option A: JSON-based interchange

1. The requesting application queries the database
2. It formats the results as JSON,
3. It then stores it in the report job queue database, in the parameters JSON field
4. The generating application passes the record pointer to JasperReports
5. JasperReports queries the database, using the database's JSON functions

#### Advantages

Simple schema and no dynamic table creation. There is only one field (`parameters`) in the database, no need for per-report schema changes.

The report job queue database remains lightweight

* No need to create/drop tables dynamically.
* Only stores a small number of report_jobs records at a time.

JasperReports Can Natively Query JSON (and the underlying database supports it)

* PostgreSQL: jsonb indexes allow efficient filtering.
* SQLite (3.38+): json_extract() and json_each() functions for iteration.

Easier Multi-Application Interchange

* JSON is universally supported, making it easier for applications to generate reports.

#### Disadvantages

Memory and CPU intensive for large datasets

* Large JSON blobs must be read fully into memory before being processed.
* Querying JSON inside SQLite is slower than in PostgreSQL, as there is no native indexing.

There is a parsing overhead in JasperReports

* If the report-generating application parses JSON, it uses extra memory.
* If JasperReports queries JSON directly, the performance depends on the database’s JSON support.

Potential Size Limits

* Some database engines impose JSON field size limits.
* Large JSON blobs may cause slow I/O operations when reading/writing.

Difficult to Index & Query Efficiently

* Joins on JSON fields are inefficient.
* Extracting structured data from JSON increases query complexity.

<!-- livebook:{"break_markdown":true} -->

### Option B: Database table-based interchange

1. The requesting application queries the database
2. It creates a structured table in the database, on the fly
3. It inserts query results as rows
4. The report-generating application reads the job, passing a reference to the table to JasperReports
5. JasperReports queries the table directly using SQL
6. The table is cleaned up, or completely dropped, after report generation.

#### Advantages

High performance for large datasets

* No JSON parsing overhead.
* Queries use native indexing and relational optimizations.

Optimized memory usage

* Data is streamed row-by-row, avoiding large in-memory blobs.
* Queries can filter, aggregate, and join efficiently (if necessary).

Minimal processing in the report-generating application

* JasperReports reads structured SQL data directly.
* No need for the report-generating application to parse JSON or transform data.

Scales to millions of records

* Structured tables handle thousands/millions of rows efficiently.
* Supports batch inserts and indexed queries.

#### Disadvantages

Schema management overhead

* Requires creating/dropping tables dynamically OR
* Reusing a fixed table and deleting old records (which may cause fragmentation).

Slightly more complex query logic

* Requires defining table structures per report type.
* Queries must match expected table formats.

Concurrency considerations

* Multiple reports running in parallel need safe table handling.
* Requires transactional integrity if updating/deleting rows after processing.

<!-- livebook:{"break_markdown":true} -->

### Evaluation

| Criteria               | Option A: JSON-Based                     | Option B: Table-Based           |
| ---------------------- | ---------------------------------------- | ------------------------------- |
| Data Exchange Speed    | Fast for small JSON, slow for large JSON | Fast for all sizes              |
| Memory Usage           | High for large JSON                      | Low (row-by-row processing)     |
| CPU Load               | High (JSON parsing overhead)             | Low (SQL optimizations)         |
| Ease of Implementation | Simple (one JSON field)                  | Requires schema management      |
| Scalability            | Slows down at thousands of records       | Handles millions of rows easily |
| Indexing & Querying    | No JSON indexing (except PostgreSQL)     | SQL indexes, efficient joins    |
| Robustness             | Prone to large JSON failures             | More robust for large datasets  |
| Maintainability        | Easier for simple reports                | Requires per-report table logic |
| Failure Modes          | Parsing failure, out-of-memory errors    | Handles large datasets safely   |

<!-- livebook:{"break_markdown":true} -->

### Critical comparison

For small datasets (≤ a few hundred rows): JSON-based interchange, option A, is fine. If reports are lightweight, JSON keeps things simpler.

For large datasets (thousands to millions of rows): the database table-based interchange, option B, is the best choice. Using a database table to transfer data avoids the JSON parsing overhead, is efficient, scalable, and robust, and as JasperReports natively supports SQL queries, this makes it seamless.

JSON is best for simple and light reports and for use in the early stages of report development, prototyping. For large reports, and eventually all reports, use of structured tables will be better in every way, after the initial table-creation cost is paid.

<!-- livebook:{"break_markdown":true} -->

The database table-based interchange, option B, will be the primary method for reports, as it will be faster, scale better, avoiding memory issues.

```
•   JSON-based (Option A) should be a fallback for small, ad-hoc reports or cases where the dataset structure is highly variable and doesn’t justify schema changes.
```

<!-- livebook:{"break_markdown":true} -->

1. Performance at scale
   * JSON parsing is CPU-intensive, especially with large datasets.
   * Table-based data streams row-by-row, keeping memory use low.
   * Indexes, joins, and filters work efficiently in SQL, whereas JSON queries are slower and lack indexing (except in PostgreSQL).
2. Memory and stability
   * JSON requires loading the full dataset into memory. Large reports can crash if JSON blobs become too big.
   * Table-based storage allows querying and pagination without excessive memory use.
3. Supports large and complex reports
   * When generating reports with thousands of rows, JSON will become a bottleneck.
   * Structured tables handle aggregations, relationships, and pagination efficiently.

<!-- livebook:{"break_markdown":true} -->

### When to Use JSON as a Fallback

There are cases where JSON is useful:

* If reports involve highly dynamic data (e.g., nested structures that are unpredictable).
* When prototyping without modifying schemas.
* If only small amounts of data are stored in parameters, and rewriting the structure for a table doesn’t make sense.

However, JSON should be used only when necessary and for small datasets. If a JSON-based report consistently exceeds a few hundred rows, it should be converted to a table-based approach.

#### Implementation Strategy

1. Default
   * Use database table-based storage for structured reports.
   * Each report type has a dedicated table
   * JasperReports queries these tables directly.
2. Fallback
   * Allow JSON-based storage only for small or highly dynamic reports.
   * If JSON is used, JasperReports will query JSON directly
3. Automation
   * When inserting report data, choose table or JSON automatically based on dataset size or structure.
   * Keep JSON for datasets under a few hundred rows, and switch to structured tables otherwise.

## Dynamic report data table creation and cleanup

1. When a report request is created, the requesting application:
   * Examines the dataset structure.
   * Dynamically generates a table in the shared database.
   * Inserts the dataset into the generated table.
   * Triggers the report generation process.
2. After the report is generated, the report-generating application:
   * Reads the dataset from the dynamic table.
   * Generates the report.
   * Drops the temporary table, or rows of data used, to free up resources.

### Step-by-step implementation

When a report request is made, the requesting application should:

1. Inspect the dataset (list of maps).
2. Infer column names and types.
3. Generate and execute a CREATE TABLE statement.
4. Insert the dataset into the new table.
5. Store the table name in the `report_requests` record so that the report-generating application knows where to look.

<!-- livebook:{"break_markdown":true} -->

This solution brings about many benefits, for one, it is fully dynamic; there is no need for predefined table structures as tables are created as needed. It is efficient as SQL queries will be used directly, instead of in-memory JSON parsing on both ends of the implementation, also making for much-reduced memory usage. After every job, there is a fast and complete cleanup, as tables are dropped as soon as they're no longer needed. Lastly, and very importantly, the report generator application remains independent as new report types don’t require application logic or storage modifications. In this way, any new reports that are developed don't force any changes to be developed in the report generator, keeping it small, simple and generic, all massive wins through reduced costs of future maintenance.

```elixir
dataset = [                                                                                                                       
  %{                                                                                                                    
    id: "eb66b8f4-c26f-446f-84a1-c163e5e28e7e",                                                                         
    description: "Testing with non-billable timer",                                                                     
    modified: 1,        
    tags: "[null]",                                         
    duration: 1,                                            
    inserted_at: ~N[2025-03-20 01:11:01],
    updated_at: ~N[2025-03-20 01:11:19],
    start_stamp: "2025-03-20T01:11:01",
    billing_duration: 0,  
    duration_time_unit: "minute",
    end_stamp: "2025-03-20T01:11:19",                     
    project_name: "",                                       
    business_partner_name: "",                                                                                          
    activity_type: "",                                      
    activity_type_id: nil,
    billable: false,       
    billing_duration_time_unit: "thirty_minute_increment",
    billing_rate: Decimal.new("0"),
    business_partner_id: nil,                               
    project_id: nil,
    end_time: "01:11",
    end_date: "2025-03-20",
    start_time: "01:11",
    start_date: "2025-03-20",
    billing_duration_in_hours: 0.0
  },
  %{
    id: "4f281577-3501-4687-8cfc-7340641e6489",
    description: "The quick brown fox jumped over the lazy dogs",
    modified: 1,
    tags: "[null]",
    duration: 1,
    inserted_at: ~N[2025-03-21 14:15:35],
    updated_at: ~N[2025-03-21 14:16:02],
    start_stamp: "2025-03-21T14:15:35",
    billing_duration: 0,
    duration_time_unit: "minute",
    end_stamp: "2025-03-21T14:16:02",
    project_name: "UI research",
    business_partner_name: "",
    activity_type: "",
    activity_type_id: nil,
    billable: false,
    billing_duration_time_unit: "thirty_minute_increment",
    billing_rate: Decimal.new("0"),
    business_partner_id: nil,
    project_id: "deb4057f-539a-42bb-91ad-7884e59a666e",
    end_time: "14:16",
    end_date: "2025-03-21",
    start_time: "14:15",
    start_date: "2025-03-21",
    billing_duration_in_hours: 0.0
  },
  %{
    id: "ab1c9580-d9ce-4027-b7e4-ed00b89fec8a",
    description: "Taking the statues for a walk",
    modified: 1,
    tags: "[null]",
    duration: 61,
    inserted_at: ~N[2025-03-21 21:14:05],
    updated_at: ~N[2025-03-21 21:14:15],
    start_stamp: "2025-03-21T20:14:05",
    billing_duration: 3,
    duration_time_unit: "minute",
    end_stamp: "2025-03-21T21:14:15",
    project_name: "Fuckabout",
    business_partner_name: "OCHM",
    activity_type: "",
    activity_type_id: nil,
    billable: true,
    billing_duration_time_unit: "thirty_minute_increment",
    billing_rate: Decimal.new("0"),
    business_partner_id: "90bc20d3-be65-46ea-a579-453d6ae3d378",
    project_id: "3fa969fe-bdf7-4f9b-af29-7ea506531996",
    end_time: "21:14",
    end_date: "2025-03-21",
    start_time: "20:14",
    start_date: "2025-03-21",
    billing_duration_in_hours: 1.5
  }
]
```

```elixir
columns = [
  id: "TEXT",
  description: "TEXT",
  modified: "INTEGER",
  tags: "TEXT",
  duration: "INTEGER",
  inserted_at: "TEXT",
  updated_at: "TEXT",
  start_stamp: "TEXT",
  billing_duration: "INTEGER",
  duration_time_unit: "TEXT",
  end_stamp: "TEXT",
  project_name: "TEXT",
  business_partner_name: "TEXT",
  activity_type: "TEXT",
  activity_type_id: "TEXT",
  billable: "BOOLEAN",
  billing_duration_time_unit: "TEXT",
  billing_rate: "MAP",
  business_partner_id: "TEXT",
  project_id: "TEXT",
  end_time: "TEXT",
  end_date: "TEXT",
  start_time: "TEXT",
  start_date: "TEXT",
  billing_duration_in_hours: "REAL"
]
```

```elixir
column_names = columns |> Enum.map(fn {field, _} -> field end)
```

```elixir
comma = ", "
insert_statement = "INSERT INTO report_client_timesheet_5fae0880_0f4f_4f06_a6fd_b48f0cba539b (#{Enum.join(column_names, comma)}) VALUES "
```

```elixir
defmodule Typer do
  # def type(value) do
  #   IEx.Helpers.i value
  # end

  def type(value) when is_number(value), do: value
  def type(value) when is_struct(value, NaiveDateTime), do: NaiveDateTime.to_string(value)
  def type(value) when is_struct(value, DateTime), do: DateTime.to_string(value)
  def type(value) when is_struct(value, Date), do: Date.to_string(value)
  def type(value) when is_struct(value, Decimal), do: Decimal.to_string(value)
  def type(true), do: 1
  def type(false), do: 0
  def type(value) when is_bitstring(value), do: value
  def type(value) when is_nil(value), do: ""
end
```

```elixir
insert_values =
  Enum.map(dataset, fn record ->
    values =
      Enum.map(column_names, fn col_name ->
        Map.get(record, col_name) |> Typer.type()
      end)

    "(#{Enum.join(values, ", ")})"
  end)

# |> Enum.join(", ")
```

```elixir
# insert_sql_statement = insert_statement <> insert_values
```

## Temporary in-memory databases

Instead of creating temporary tables with unique table names, then _dropping_ them once the requested report has been generated, a viable and possibly cleaner solution is to use SQLite's in-memory database feature. The elegance provided by this solution is perfect for the needs of the project: by storing the reports' input datasets in in-memory database tables—instead of on disk—performance could not be faster; secondly, once the connection to an in-memory database is closed, data is automatically purged, simplifying table management and automatically ensuring privacy.

<!-- livebook:{"break_markdown":true} -->

The research into this seductive possibility was short-lived, however, as SQLite provides two kinds of in-memory databases, with the inherent limitation that the database could only be accessed from the _same operating system process_. This immediately means that both types of in-memory databases provided by SQLite can not be used for inter-process communication, which is exactly what we're trying to achieve.

<!-- livebook:{"break_markdown":true} -->

Instead of pursuing this and creating endless complications to solve—which is doable, but impractical—it's more valuable to spend the time creating a robust temporary table creation and tear-down infrastructure. The simplest solution to having to _remember_ to delete temporary tables is the use of triggers, reacting to the change of status of the job, such that once a job is marked as 'complete', a trigger will immediately delete all tables containing the unique ID of the queued report job.

## Defining possible job states

In order to build post report-generation functionlity, as well as any other work based on the state of the report job, we must specify all possible states. A project that has done this job well is [Oban](https://hexdocs.pm/oban/Oban.html), Elixir's backgound job framework, so I will use states defined as legal in that project as a starting point.

<!-- livebook:{"break_markdown":true} -->

### Job State Transitions (taken directly from [Oban documentation](https://hexdocs.pm/oban/Oban.Job.html#states/0))

`:scheduled` Jobs inserted with `scheduled_at` in the future are `:scheduled`. After the `scheduled_at` time has elapsed the `Oban.Plugins.Stager` will transition them to `:available`

`:available` Jobs without a future `scheduled_at` timestamp are inserted as `:available` and may execute immediately

`:executing` Available jobs may be run, at which point they are `:executing`

`:retryable` Jobs that fail and haven't exceeded their max attempts are transitioned to `:retryable` and rescheduled until after a backoff period. Once the backoff has elapsed the `Oban.Plugins.Stager` will transition them back to `:available`

`:completed` Jobs that finish executing succesfully are marked `:completed`

`:discarded` Jobs that fail and exhaust their max attempts, or return a `:discard` tuple during execution, are marked `:discarded`

`:cancelled` Jobs that are cancelled intentionally

<!-- livebook:{"break_markdown":true} -->

#### TODO

* [ ] Once the reporting prototyping is complete, and the solution is ready for production, carefully document all the above states and their meaning
* [ ] In future, consider the possibility of a complete refactor of the job queue table, considering whether it's possible and desirable to delegate all job queue recording and management tasks to Oban, a it is a dedicated and continually maintained solution

## Improving performance through memoisation

[Memoisation](https://en.wikipedia.org/wiki/Memoization) is a performance/resource use optimization technique used primarily to speed up computer programs by storing the results of expensive operations, returning the cached result when the same inputs are used. It is a type of caching.

<!-- livebook:{"break_markdown":true} -->

In this context, generating reports is a relatively _expensive_ operation, requiring the calling of another environment, hosting the report-generating environment, using a multiple of the memory required to handle the data fitting the criteria, as the data undergoes several transformations:

1. first retrieval and _shaping_ from the source database
2. transformation into a JSON structure, and storage in the target format by the requesting application
3. retrieval from storage by the report-generating application
4. parsing back into native data structure
5. final conversion into structure necessitated by report-generating library

After which the report generation itself is a demanding process which needs to cleverly typeset the data, calculating correct line-breaking and page-breaking in the process—using yet more memory and CPU—finally _exporting_ it to the destination format (PDF, word processor document, spreadsheet, etc.)

<!-- livebook:{"break_markdown":true} -->

All of this takes time, using system memory and processing power fairly intensively. Simplifying reports, amount of data, interprocess communication (see above), and other optimisations will result in some resource savings, but cannot deliver significant impact on overall system use. But first, when surrounded by such powerful computing capacity, why should we concern ourselves with optimisation of any kind? For practical reasons: all computing power has a discernible cost attached. Ideally, we want to use the least computing power that we can get away with, and pay the lowest reasonable price. For example, let's assume that a simple application can usefully and reliably be hosted on a $5 per month computer (VPS). By prioritising a _pragmatic_ optimisation strategy, we can use this resource to do _more work_, or to support _more simultaneous users_, both of which are useful goals.

<!-- livebook:{"break_markdown":true} -->

So, how can we deliver a major multiple of optimisation, particularly in an environment used by multiple people? _Memoisation_ of generated reports, by which I mean remembering previously requested reports. It is a reality that the same report will be requested again, either by the same person who has forgotten that they've already had that particular report generated, or in situations where they've lost track of the generated report file, which they now need again. In contexts where multiple people use the same system, it is very likely that different people will request the same report, for example when a customer contacs the company, requesting confirmation of, or a duplicate of, a statement or other report. In both of these cases, we'll consider the regeneration of the same report _wasteful_, and will find it more economical to keep a log of generated reports for later retrieval, than reissuing report generating requests.

<!-- livebook:{"break_markdown":true} -->

By storing key information, such as all input _criteria_ selected when generating reports, as well as the full set of data which goes on to make up the report, including certain other metadata, we can easily detect when a report has been requested that's _identical_ to one already generated in the past. We can refer to this as a _report fingerprint_, as the following _uniquely_ identifies any kind of report we can generate:

* Report name (customer statement, activity report, invoice)
* Report style (which report template was requested)
* Output format (PDF, spreadsheet, CSV)
* Report criteria (date range, customer name, project)
* Report data (the full dataset making up the report)
* Any other _metadata_ used (notes, name of person requesting the report)
* _Freshness_ (when was this last generated, is having the latest date on the report crucial)

<!-- livebook:{"break_markdown":true} -->

Using this methodology means that every report will have to be generated _once_. Subsequently, when any person using the system makes a request for a report, resulting in an identical _report fingerprint_ to one already in the report log, instead of going through all the steps of actually generating the report, the system cn simply provide the user with the same report it has already provided once. As a secondary advantage, this log serves as an audit trail for all the organisation's report requirements, providing an easy way to calculate the impact of this strategy, reporting statistics on time and resources saved. Intuitively, it is my opinion that overall efficiencies of up to 25% are achievable in an average organisation.

<!-- livebook:{"break_markdown":true} -->

### How Report Memoization Works

Instead of generating the same report many times:

1. Compute a unique fingerprint for each report request, based on input parameters and the provided dataset.
2. Check the database before inserting a new job. If an identical report exists, return the previously generated report.
3. Only generate new reports when no previous version exists.
4. Automatically expire old reports based on a retention policy.

<!-- livebook:{"break_markdown":true} -->

### Expiring Old Reports

To prevent indefinite keeping of reports, we can set a retention policy:

* Expire reports after X days by deleting old jobs.
* This can be done with a periodic cleanup query in either the calling or report generating application, though the former is more sensible.

<!-- livebook:{"break_markdown":true} -->

### Advantages of memoisation

| Feature          | Without Memoization        | With Memoization                                |
| ---------------- | -------------------------- | ----------------------------------------------- |
| Performance      | Re-generates every report  | Avoids redundant work                           |
| User Experience  | Slow for repeated requests | Faster report retrieval                         |
| CPU/Memory Usage | Higher                     | Lower                                           |
| Storage Use      | More reports stored        | Less storage required                           |
| Fault Tolerance  | Jobs persist               | Jobs persist and can be skipped if already done |

<!-- livebook:{"break_markdown":true} -->

The first step in implementing this memoisation strategy is to reliably calculate every report's _unique fingerprint_ as described above. Doing this must be fast and _cheap_ on system resource use, while being robust, consistent, and _collision-resistant_. What is collision resistance? Mathematical operations are applied on the report criteria and input data, returning a unique line of apparently random text which uniquely identifies precisely that input. Even though they're designed against it, some known mathematical operations—or /algorithms/—are prone to return the exact same result given two different inputs. Even though this application is not being designed for security or privacy purposes, the application would provide a very poor user experience (UX) if it were to return a completely different report than the one requested! Hence, there is a tradeoff to be considered between an algorithm that needs fewer resources to compute its result, while providing a higher guarantee against collision resistance.

## Choosing a hash function for unique fingerprinting

_Hash functions_ are the mathematical operations described above. Given the specified criteria, there's a need for a hash function with a strong collision resistance—ideally _never_ resulting in a collision—pragmatically balanced against the speed of operation and consumption of computing resources. There are several commonly implemented hash functions available for consideration: SHA1, SHA2, SHA3, BLAKE2, SM3, MD4 and MD5.

<!-- livebook:{"break_markdown":true} -->

### MD4 and MD5

Speed: Very fast
Resource usage: Low
Collision resistance: Poor - both have been broken cryptographically
Output size: 128 bits (16 bytes)
Notes: Though fast, both have demonstrated collisions and are not recommended even for non-security fingerprinting

### SHA1

Speed: Fast
Resource usage: Low to moderate
Collision resistance: Compromised (collisions have been found)
Output size: 160 bits (20 bytes)
Notes: Better than MD4/5, but still vulnerable to collisions

### SHA2 (SHA-256/SHA-512)

Speed: Moderate
Resource usage: Moderate
Collision resistance: Strong
Output size: 256/512 bits (32/64 bytes)
Notes: Widely implemented, good balance of performance and collision resistance

### SHA3

Speed: Slower than SHA2
Resource usage: Higher than SHA2
Collision resistance: Very strong
Output size: Variable (224-512 bits)
Notes: Newer design but unnecessary computational overhead for fingerprinting

### BLAKE2

Speed: Very fast (often faster than MD5)
Resource usage: Low
Collision resistance: Strong
Output size: Variable (up to 512 bits)
Notes: Optimized for software implementation, especially on modern CPUs

### SM3

Speed: Similar to SHA-256
Resource usage: Moderate
Collision resistance: Strong (no known practical collisions)
Output size: 256 bits (32 bytes)
Notes: Chinese standard, less widely implemented in common libraries

<!-- livebook:{"break_markdown":true} -->

On balance, [`BLAKE2`](https://www.blake2.net/) is the best option, offering the following advantages:

Excellent speed (typically faster than all others except possibly MD4/MD5)
Low resource usage
Strong collision resistance
Widely available in modern programming libraries
Specifically designed for high performance in software implementations

If BLAKE2 isn't available in the desired environment, SHA-256 (part of SHA2) is the secondary recommendation as it balances good performance with very strong collision resistance.

According to the project:

> BLAKE2 is a cryptographic hash function faster than MD5, SHA-1, SHA-2, and SHA-3, yet is at least as secure as the latest standard SHA-3. BLAKE2 has been adopted by many projects due to its high speed, security, and simplicity.

<!-- livebook:{"break_markdown":true} -->

The BLAKE2 hashing function comes in two variants: `blake2b` and `blake2s`. The first, `blake2b` is faster, and is highly optimised for 64-bit CPUs. The project page shows how much faster it is, demonstrating that it beats all the other hash function considered, and that it is over four times faster than the next function offering similar collision resistance, SHA3.

## Privacy impact of memoisation

The idea driving the development of a memoisation strategy to improve performance, while building a log of requested and generated reports is useful on at least two fronts: increased speed/reduced resource use, an audit trail, report-generation telemetry which can be statistically analysed for future improvements. However, this benefit comes with the ability to negatively impact privacy.

<!-- livebook:{"break_markdown":true} -->

On the one hand, data which can be protected by the primary application database is now exfiltrated to a secondary database—and consuming application—which has to be trusted.

Secondly, this data, as well as the criteria used to shape it, remains in the database after the report has been generated. The data, along with the generated report, serves as a snapshot in time, resisting modification or any subsequent deletion in the primary application, which could become problematic.

<!-- livebook:{"break_markdown":true} -->

The first issue simply cannot be fully secured against, if we are to share this data to generate reports. Building the Reporter application securely, to the highest quality standards of the day, is the best way of ensuring that nothing negative is likely to happen to the data. Ultimately, though, this data is still shared with the Jasper Reports library, which ultimately has to be trusted.

<!-- livebook:{"break_markdown":true} -->

The second issue is far easier to alleviate. Data must remain in the table only for as long as it takes to generate the report, or give up and cancel the job. Once either of those two conditions have been met, the report _parameters_, or criteria, as well as the dataset itself, may be safely expunged. The persistence of the parameter and dataset fingerprints is enough to prevent unnecessary report regeneration in the future, providing the central space-for-time performance-optimising promise of memoisation for the lifetime of the log. Addressing resistance to change long after the report is generated, if the same report is requested in the future, possessing the same parameter fingerprint but with a differing dataset fingerprint, the legacy record can finally be _invalidated_, with the generated report expunged, reducing any impact of the _snapshot in time_ weakness.

<!-- livebook:{"break_markdown":true} -->

As the generated report must be kept around to provide those performance guarantees, the privacy aspect can _never be fully assured_, as one look at the report will provide the data to a motivated party. However, we must keep in mind that the primary methods of report dissemination—printing to hand out, and sending by email—are not in themselves secure, allowing analysis long into the future through these channels, thus providing important context to this risk. All that is to say, generated reports contain confidential data which can only be secured by the system so far and no more.
