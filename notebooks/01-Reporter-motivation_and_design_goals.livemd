<!-- livebook:{"autosave_interval_s":60} -->

# Reporter: Goals, motivation, methodology and design

## Abstract

_Reporter_ is an application designed for the production of professional enterprise reports. A critical and expected component of software applications performing important functions in businesses is the generation of dynamic PDF reports, for printing or sharing with other people. What is key is that reports must be customisable, aesthetically pleasing, professionally typeset, and print-ready, in case of PDF production, or consumption-ready in cases of other office-suite reports needing to be consumed as an intermediate format.

<!-- livebook:{"break_markdown":true} -->

_Reporter_ is a report-generation wrapper around the Jasper Reports library. The intent behind this project is to completely sidestep the need to program in Java for generating professional reports. Clojure, a Lisp family of languages, is built on Java and hosted on that virtual machine, which gives it access to decades of enterprise-level applications that run on the Java virtual machine, while _improving_ on a number of Java's rough edges, opening it up to a pleasing programming environment—to those who love working in Lisp.

## The everlasting need for reports

Reports are the lifeblood of a corporation, finding use in board discussions where it is unwieldy to rally participants around a shared computer screen, or to present interactions with an application via a projection; in doling out tasks to employees and subcontractors; in synthesising thoughts and plans of action, over carefully formatted and distilled figures on a printed report, without the distraction of the rest of the application, a means to achieve higher thought processes. There are many reasons not covered by this list, but the point stands: reports are not going away and must be given the respect they deserve in professional settings.

<!-- livebook:{"break_markdown":true} -->

There are many types of report needed, depending on the activity in question, the person or people the report targets, the action which the report should instigate, and many more. Below is a non-exhausive list of reports needed on a regular basis:

<!-- livebook:{"break_markdown":true} -->

* Invoices
* Proof of receipt
* Statements of transactions
* List of outstanding payments, monies owed
* Financial statements, company accounts
* Sales reports
* Marketing performance reports
* Customer relationship management reports
* Lists of tasks outstanding
* Lists of actions undertaken, work carried out
* List of schedules, e.g. regular maintenance, consumable replacement, safety checks due
* Inventories taken and recorded on particular dates
* Summary of communications with third parties
* Specifications of a product, service or asset
* Marketing materials for the above
* Resource allocation map, e.g. available properties, potential tenants
* Manifest
* List of assets

## Report-generating solutions considered

Report generation is a very important function of all serious software efforts. The most common requirement is the creation of PDF reports, but a few related formats are often desirable in commercial settings, including creation of comma-separated values (CSV) exports, word processor documents (Word, Pages, LibreOffice Writer), spreadsheets (Excel, Numbers, LibreOffice Calc), presentations (PowerPoint, Keynote), as well as others.

<!-- livebook:{"break_markdown":true} -->

In consideration of plausible solutions to the requirement, the most important criteria had to be met:

* A visual report template designer is a must, so that non-developers could use this to modify existing templates for their needs
* An API for generating finished documents
* Support for dynamic content generation
* Rich text formatting capabilities
* Generation of PDF as a minimum, as well as some of the target document types described above
  * That the PDF generation isn't merely a glorified HTML to PDF solution, such as `wkhtmltopdf`, as HTML isn't yet a powerful typesetting and layout solution
  * Pixel-perfect design precision
  * Page-aware layout engine
  * Print-oriented typesetting
    * Quality typesetting, preferrably with high-quality justification
    * Different first pages
    * Document structure, including:
      * Different page sizes within the same document
      * Section-specific headers and footers
      * Page numbering that can reset between sections
      * Background layers that can vary by page type
  * Print-ready output (desirable, not mandatory)
    * CMYK color space (important for professional printing)
    * Embedded fonts
    * High-resolution image embedding
    * PDF/A compliance for archiving
    * Proper bleed and margin handling
  * Predictable rendering
* Reduced resource use, higher-performance report generation

<!-- livebook:{"break_markdown":true} -->

Solutions considered:

1. Jasper Reports
2. Pentaho
3. BIRT
4. QuestPDF

<!-- livebook:{"break_markdown":true} -->

Any solution to be considered had to meet a few desiderata to be taken seriously:

* The project is open-source
* The project is alive and healthy, as witnessed by the recency and frequency of commits to their repository
* A usable level of documentation exists, so that potential users are not expected to chase for basic information in forums and issues

<!-- livebook:{"break_markdown":true} -->

The underlying expectation in the selection process was that any solution chosen was likely to be in a very foreign programming language, most likely to be written in one of the corporate _darlings_: Java, .NET, C#, JavaScript, and would have to be _adapted_ in some way to work with in-house preferred languages: Elixir, Clojure, Common Lisp, OCaml. This drove the decision to wrap the generator within one of the in-house languages.

<!-- livebook:{"break_markdown":true} -->

I selected Jasper Reports as it clearly meets many of the criteria laid out; its visual designer Jaspersoft Studio was fairly easy to use intuitively; it had a comprehensive project manual with which to get started. As Jasper is written in Java, Clojure is a pragmatic choice as the wrapper language, as I understand it and trust a number of the guarantees it provides, and the developer experience it affords.

## Initial report prototype

One of the important reasons driving the choice for Jasper is that it has a graphical user interface application—Jaspersoft Studio—making it possible for non-developers to design and modify reports with their drag and drop interface.

<!-- livebook:{"break_markdown":true} -->

To demonstrate the feasibility of Jaspersoft Studio, a sample report template was designed, a client timesheet, defined to take in a file containing a list of JSON-formatted data records, for use as the report's data source. Entirely using Studio, the template was designed, the datasource registered, and the feasibility of the graphical interface proved itself fully capable, making it easy to generate incremental finished reports through the _Preview_ facility. Rounding it off, Studio makes it trivial to export the template under design, using the defined datasource, directly to one of fourteen different formats.

<!-- livebook:{"break_markdown":true} -->

In the early prototyping stage, Studio made it easy to finesse the demonstration timesheet report, finally exporting it to PDF, all from the visual user interface. The relative simplicity of the entire prototyping step proved that Jasper—the solution—meets all pragmatic project needs, determining that the entire solution would be architected around the Jasper Reports library.

## Inter-process communication

With the initial hypothesis now proven, that Jasper Reports can generate pleasing and rich PDF reports, the next step is to start building a solution _wrapping_ all available functionality in a language and environment more amenable to me. Clojure is my answer to wrapping Java libraries, given that the language is hosted on the Java virtual machine. The question needing an answer now is how a requesting process would formally make the initial request—monitoring its progress and outcome—of the report-generating solution?

<!-- livebook:{"break_markdown":true} -->

Several methods are available:

1. Local communication (e.g., file-based or HTTP server) works well for simple, local use cases.
2. Distributed communication using HTTP APIs, message queues, or GRPC is more scalable and suitable for remote or cloud-based environments.
3. Containers provide flexibility and ease of scaling with orchestration tools like Docker Compose or Kubernetes.

I am not a fan of containers, so will not further two method 3. Breaking the other two methods down provides further finer choices of direction:

1. Local Interprocess Communication (IPC)

   1. **File-based Communication** The requesting application can write data to a file (such as JSON or CSV) and invoke the report-generating service, which reads the file, generates the report, and returns it. This is useful for simple, one-off reporting tasks.

      **Pros**:

      * Simple to implement for small-scale, local tasks.
      * Easy to debug and monitor.

      **Cons**:

      * Not very scalable.
      * Could lead to data synchronization issues in high-frequency or real-time scenarios.

   2. **Local HTTP Server** The report-generating service can expose an HTTP API using libraries like Ring or http-kit, and the requesting application can make HTTP requests to this local service.

      **Pros**:

      * Easy to scale out (you can add more report-generating instances behind a load balancer).
      * Can be easily monitored and controlled.

      **Cons**:

      * More complex than file-based communication.
      * Slightly more overhead with HTTP, but still suitable for local use.

2. Remote Communication (Distributed Services)

   1. **HTTP API (RESTful or GraphQL)** Expose a RESTful API or GraphQL endpoint in the report-generating application, where the requesting application can send HTTP requests to pass the data. This is the most flexible and scalable solution, as it allows you to separate concerns and deploy each service independently.

      **Pros**:

      * Highly scalable: You can run multiple instances of the report-generating service.
      * Can be made stateless and cloud-friendly.
      * Easily monitored using common observability tools.

      **Cons**:

      * Requires more infrastructure (e.g., load balancers, monitoring, etc.).

   2. **Message Queue (e.g., RabbitMQ, Kafka, etc.)** The requesting application can send messages (data for the report) to a message queue, and the report-generating can consume these messages to generate reports. This allows for asynchronous communication, better decoupling, and scaling. This works well if you need to queue multiple requests or manage high volumes of requests.

      **Pros**:

      * Asynchronous and scalable.
      * Handles retries and message persistence.
      * Works well for high-volume use cases.

      **Cons**:

      * More infrastructure required (e.g., RabbitMQ, Kafka).
      * Slightly more complex setup.

   3. **GRPC (For High-Performance, Low-Latency Communication)** For highly performant, low-latency communication, you can use gRPC (a remote procedure call framework). Both Elixir and Clojure have gRPC libraries that allow for bidirectional communication between services.

<!-- livebook:{"break_markdown":true} -->

The variable is the scale and requirements of the application, here are some options:

1. Local communication (e.g., file-based or HTTP server) works well for simple, local use cases.
2. Distributed communication using HTTP APIs, message queues, or GRPC is more scalable and suitable for remote or cloud-based environments.

<!-- livebook:{"break_markdown":true} -->

Judging the requirements on their own merit, the gRPC approach is technically _the best_, with the local file-based method a more pragmatic second.

<!-- livebook:{"break_markdown":true} -->

I'm a strong believer in simplicity, and while the distributed option has many benefits—distribution and ability to scale onto different and multiple systems, as well as the inherent volume and capacity benefits of that approach—the complexity of running this as a distributed service is not justifiable for all but the very most demanding environments. The local solution is better on many levels, including through simplified management, rollout and continued maintenance of the solution. I believe that this simplicity and the time savings to be reaped from it far outweigh the startup time of the application each time a report is requested, the _warmup_ of the solution, and the time taken for the delivery of the report. What's more, I think that the expectation—and need—for instant reports, simply doesn't exist; for the few reports that are needed (as a fraction of all functionality delivered) a reasonable waiting time is perfectly acceptable.

<!-- livebook:{"break_markdown":true} -->

The way forward, then, is to develop a local communication method, using the filesystem or similar method, to handle the volume that is expected, and in the future this solution can be extended to also support distributed communication, distribution and scaling. Though GRPC is seductive, the simpler HTTP method is better understood and supported across all architectural levels of the application.

## Local inter-process communication strategy and its benefits

#### Communication complexity

* **Complexity** A file-based approach is relatively simple to implement as it relies on basic file I/O operations (reading and writing to disk). The requesting application would write data to a file (in JSON, CSV, or other consumable format), and the report-generating application would read that file, generating the report, writing the output to a file which will be picked up by the requesting application.
* **Batch Processing** This approach is not “real-time” in the way that a gRPC method of communication would afford. Data is first written to a file, then later processed, making this less suitable for low-latency or real-time applications.
* **Asynchronous Nature** File-based communication is naturally asynchronous, as the calling application can spawn new processes independently handling report generation. However, this requires proper synchronization or polling to ensure the calling application knows when the report is ready.
* **Scalability** File-based communication doesn’t inherently scale out across multiple services, containers or servers, and its scalability would depend on how the file I/O system is designed and managed (e.g., shared network drive, cloud storage, etc.).

<!-- livebook:{"break_markdown":true} -->

#### System design and workflow

* **Loose Coupling** File-based communication is typically more loosely coupled. The requesting and generating applications don’t need to be aware of each other directly, beyond the files they exchange. The requesting application can write reports asynchronously, and the report-generating application can process them when available.
* **Process Independence** The requesting application can run report generation processes in parallel (by spawning multiple processes) and check files at intervals for completion, allowing the services to work independently.
* **No Dependency on Running Services** If the report-generating application isn’t running at the moment, the requesting application can still generate files or trigger report generation jobs, providing some flexibility. If the system goes down, it’s easier to restart the processes without a tight dependency chain.

<!-- livebook:{"break_markdown":true} -->

#### Error handling and failures

* **Error Handling** In a file-based system, error handling would likely be manual. If something goes wrong, you’ll need to handle retries or notify the system to take corrective actions.
* **Fault Tolerance** File-based approaches may be more tolerant to failures in the sense that both the requesting and generating applications can fail independently without causing a complete system failure. If the generator crashes, the requestor can wait and retry the file generation process.
* **Logging** Logging and monitoring in a file-based system can be more challenging. You need to ensure proper logging of file operations, and may need a manual or custom process to track file generation status (e.g., polling the filesystem).

<!-- livebook:{"break_markdown":true} -->

#### Performance and latency

* **Higher Latency** File-based communication inherently involves higher latency due to disk I/O and file reading/writing. The report generation process depends on file availability and may involve some polling or file system watching, which takes longer than the immediate RPC approach of gRPC.
* **Synchronous/Asynchronous Blocking** the requesting application will be blocked while waiting for the report generation process to complete if polling is used. If the file I/O system is slow, it can add additional delays.

<!-- livebook:{"break_markdown":true} -->

#### Scalability and high availability

* **Limited Scalability** Scaling with file-based communication requires careful consideration of file storage (e.g., shared network storage, cloud-based storage) and managing access to these files concurrently. You might need a distributed file system or cloud file storage to handle multiple instances of the report generation process.
* **Backup Systems** For remote backup, file-based systems are easy to implement by copying files to another service, but it may not be as seamless or efficient as the automatic failover provided by a gRPC-based system.

<!-- livebook:{"break_markdown":true} -->

#### Infrastructure and Maintenance

* **Minimal Infrastructure** A file-based approach requires minimal infrastructure. At minimum, a shared file system (local or cloud-based) is needed, where the requesting application can drop the data for the report-generating application to pick it up. If the system is small, you can run it without additional network or server configuration.
* **Scaling via Filesystem** Once you need to scale, use cloud storage services (e.g., AWS S3) or a network-attached storage (NAS) device to hold the files.

<!-- livebook:{"break_markdown":true} -->

#### Key Advantages of File-based Approach Over gRPC for Failure Management

* **Failure isolation** In the file-based approach, the requesting and generating applications can fail independently. If the generator fails, the requestor can simply restart it without being blocked or needing complex error handling. On the other hand, in gRPC, if the generating service goes down, the requestor will be impacted by this failure until the service is restored.
* **Decoupled failure handling** The file-based approach provides much more decoupling between the applications. If one process fails (either of the applications), the other can continue running, and the requestor can handle errors by either retrying or spawning new processes independently. gRPC, however, relies on real-time connectivity between both services, so failure in one service immediately affects the other.
* **Minimal external infrastructure needed** With file-based communication, you don’t need to implement complex monitoring systems (like Prometheus, health checks, etc.). The requesting application can directly manage the generator process lifecycle. For gRPC, you’d typically require additional tools for health checks, load balancing, or retries to make sure communication is resilient.
* **Simplicity** File-based communication is often simpler to reason about. With file I/O, failures tend to be easier to debug because you’re working with persistent files, and recovery or retries are less likely to depend on network connectivity issues.

<!-- livebook:{"break_markdown":true} -->

In conclusion, if the use case involves relatively few reports per day, minimal infrastructure, and asynchronous processing, the file-based approach is simpler to implement and maintain, and is sufficient. However, if you expect future growth, need low-latency and scalable communication, or plan to integrate with other systems, gRPC is a better long-term solution.

<!-- livebook:{"break_markdown":true} -->

File-based communication is a more loosely coupled and fault-tolerant approach. The requesting application has complete control over spawning and managing report—generating application—processes, making it simpler to handle failures, retries, and reinitializing processes when needed.

<!-- livebook:{"break_markdown":true} -->

In the case of failure recovery, the file-based approach excels by providing the requesting application with full autonomy over process management, including detecting failures and restarting processes as needed. The gRPC approach would require external monitoring and retry mechanisms for resiliency.

## Modifying local file-based communication to database-based

While file-based inter-process communication is simple and standard across many systems, it is different enough that it will impose per-operating system implementation modifications, due to differences—however small—between BSD, Linux, Darwin and Windows file handling subtleties. A very good substitution for file-based communication is an abstraction existing acros all the above systems and more: SQLite, the embedded relational database system. SQLite already powers billions of devices, and its role is proven in even beating native file system performance, so this research direction has significant merits, not least of which is that it presents a unified interface, _regardless_ of operating system.

Using a database-backed job queue instead of file-based communication has other significant advantages while still preserving the benefits of fault tolerance, monitoring, and process control.

<!-- livebook:{"break_markdown":true} -->

By using a separate SQLite database as a queue for pending report generation jobs, we gain:

* **Decoupling between the applications** The requesting and generating applications interact indirectly through the database instead of needing real-time connectivity like gRPC.
* **Failure resilience** Jobs persist in the queue even if either of the applications crashes, resulting in  no lost jobs.
* **Monitoring and control** The calling application can inspect the queue at any time, retry or remove failed jobs.
* **Concurrency and scaling** Multiple instances of the generating application can pick up jobs in parallel for distributed processing.
* **No file system dependencies** Unlike the file-based approach, the database avoids file-level locks, race conditions, and cleanup issues.

<!-- livebook:{"break_markdown":true} -->

#### Architecture of the SQLite Job Queue Solution

Here’s how this could work:

1. Job Insertion (requesting application):
   * The requesting application inserts a job (report request) into the jobs table in a dedicated SQLite database.
   * It sets the job status as "pending".
2. Job Processing (generating application):
   * The report-generation service polls the database at regular intervals.
   * It picks up the next available job, marks it as "processing", and generates the report.
   * Once done, it updates the job with the generated report path and marks it as "completed".
3. Result Retrieval (requesting application):
   * The requesting application checks for completed jobs and retrieves the results.
   * It can retry failed jobs, delete processed jobs, or trigger notifications when a report is ready.
4. Error Handling & Recovery:
   * If the report-generating process crashes, jobs remain in "pending" or "processing" status.
   * Requesting applications can detect stalled jobs (e.g., by using a timestamp column) and retry or reassign them.

<!-- livebook:{"break_markdown":true} -->

Below is a simple starting schema for the job queue table:

```sql
CREATE TABLE report_jobs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    status TEXT CHECK(status IN ('pending', 'processing', 'completed', 'failed')) NOT NULL DEFAULT 'pending',
    parameters TEXT NOT NULL,  -- JSON payload of report parameters
    result_path TEXT,          -- Path to generated report (filled by Clojure)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```
